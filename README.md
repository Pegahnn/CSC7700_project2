# CSC7700_project2
This project explores and compares the performance of Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTM), and Transformer-based models on a text generation task. Each model is trained using tokenized data with a Byte Pair Encoding (BPE) tokenizer built using SentencePiece.
The dataset consists of short text sequences extracted from public domain literature, specifically, 
Project Gutenberg.
